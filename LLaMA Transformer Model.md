```
┌─────────────────────────────────────────────────────────────────┐
│                        LLaMA Transformer Model                  │
└─────────────────────────────────────────────────────────────────┘

                              输入
                                ↓
┌─────────────────────────────────────────────────────────────────┐
│                    Token Embedding Layer                        │
└─────────────────────────────────────────────────────────────────┘
                                ↓
┌─────────────────────────────────────────────────────────────────┐
│                    Rotary Position Encoding                     │
└─────────────────────────────────────────────────────────────────┘
                                ↓
┌─────────────────────────────────────────────────────────────────┐
│                    Transformer Blocks (× n_layers)              │
├─────────────────────────────────────────────────────────────────┤
│  ┌────────────────────────────────────────────────────────────┐ │
│  │                    Transformer Block                       │ │
│  │  ┌─────────────────────────────────────────────────────┐   │ │
│  │  │              Residual Connection 1                  │   │ │
│  │  └─────────────────────────────────────────────────────┘   │ │
│  │                            ↓                               │ │
│  │  ┌─────────────────────────────────────────────────────┐   │ │
│  │  │                  RMSNorm (Attention)                │   │ │
│  │  └─────────────────────────────────────────────────────┘   │ │
│  │                            ↓                               │ │
│  │  ┌─────────────────────────────────────────────────────┐   │ │
│  │  │                Multi-Head Attention                 │   │ │
│  │  │  ┌─────────────┬─────────────┬─────────────┐        │   │ │
│  │  │  │      Wq     │      Wk     │      Wv     │        │   │ │
│  │  │  │ (ColumnPar) │ (ColumnPar) │ (ColumnPar) │        │   │ │
│  │  │  └─────────────┴─────────────┴─────────────┘        │   │ │
│  │  │         ↓             ↓             ↓               │   │ │
│  │  │    ┌─────────────────────────┐      ↓               │   │ │
│  │  │    │   Apply Rotary Embed    │      ↓               │   │ │
│  │  │    └─────────────────────────┘      ↓               │   │ │
│  │  │                ↓                    ↓               │   │ │
│  │  │    ┌────────────────────────┐       ↓               │   │ │
│  │  │    │ scores = torch.matmul  │       ↓               │   │ │
│  │  │    └────────────────────────┘       ↓               │   │ │
│  │  │             mask ↓                  ↓               │   │ │
│  │  │    ┌────────────────────────┐       ↓               │   │ │
│  │  │    │scores = softmax(scores)│       ↓               │   │ │
│  │  │    └────────────────────────┘       ↓               │   │ │
│  │  │                   ↓                 ↓               │   │ │
│  │  │            ┌──────────────────────────────┐         │   │ │
│  │  │            │    output = torch.matmul     │         │   │ │
│  │  │            └──────────────────────────────┘         │   │ │
│  │  │                           ↓                         │   │ │
│  │  │  ┌───────────────────────────────────────────────┐  │   │ │
│  │  │  │                  Wo (RowPar)                  │  │   │ │
│  │  │  └───────────────────────────────────────────────┘  │   │ │
│  │  └─────────────────────────────────────────────────────┘   │ │
│  │                            ↓                               │ │
│  │  ┌─────────────────────────────────────────────────────┐   │ │
│  │  │              Residual Connection 2                  │   │ │
│  │  └─────────────────────────────────────────────────────┘   │ │
│  │                            ↓                               │ │
│  │  ┌─────────────────────────────────────────────────────┐   │ │
│  │  │                  RMSNorm (FFN)                      │   │ │
│  │  └─────────────────────────────────────────────────────┘   │ │
│  │                            ↓                               │ │
│  │  ┌─────────────────────────────────────────────────────┐   │ │
│  │  │               FeedForward (SwiGLU)                  │   │ │
│  │  │  ┌─────────────┐   ┌─────────────┐   ┌───────────┐  │   │ │
│  │  │  │     W1      │   │     W3      │   │    W2     │  │   │ │
│  │  │  │ (ColumnPar) │   │ (ColumnPar) │   │ (RowPar)  │  │   │ │
│  │  │  └─────────────┘   └─────────────┘   └───────────┘  │   │ │
│  │  │        ↓               ↓                   ↑        │   │ │
│  │  │  ┌─────────┐     ┌─────────┐               │        │   │ │
│  │  │  │  SiLU   │     │  Linear │               │        │   │ │
│  │  │  └─────────┘     └─────────┘               │        │   │ │
│  │  │        ↓               ↓                   │        │   │ │
│  │  │  ┌─────────────────────────┐               │        │   │ │
│  │  │  │   Element-wise Multiply │───────────────┘        │   │ │
│  │  │  └─────────────────────────┘                        │   │ │
│  │  └─────────────────────────────────────────────────────┘   │ │
│  │                            ↓                               │ │
│  │  ┌─────────────────────────────────────────────────────┐   │ │
│  │  │                      output                         │   │ │
│  │  └─────────────────────────────────────────────────────┘   │ │
│  └────────────────────────────────────────────────────────────┘ │
│                            × n_layers                           │
└─────────────────────────────────────────────────────────────────┘
                                ↓
┌─────────────────────────────────────────────────────────────────┐
│                      Final RMSNorm                              │
└─────────────────────────────────────────────────────────────────┘
                                ↓
┌─────────────────────────────────────────────────────────────────┐
│                    Output Projection                            │
└─────────────────────────────────────────────────────────────────┘
                                ↓
                              输出
                      (仅最后一个token的logits)


***


```python
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float()).type_as(x)
        return output * self.weight


